# CrashLoopBackOff

The CrashLoopBackOff Pod restarts again and again due to 
1. misconfiguration[configmap+secrets]
2. health checks are failed wrong port and path
3. app dependecy missing [app1 connects to db]



TO check the scheduling errors
- pod placement
- init containers info
- health checks info
kubectl describe pod pod-name

For check runtime errors
kubectl logs <pod-name>

# Pod was restarting many times it create many pods and then deleted what would we the reason
becuase less computing resources causes pod restars and pod was not have enough resources to start pod so it will continuously restars still it showing running and then restarts or new pod was created

---
Perfect ðŸ‘Œ Konka â€” your observation is absolutely on the right track.
Let me refine and structure it so itâ€™s **clean and interview-ready**:

---

## ðŸš¨ CrashLoopBackOff in Kubernetes

A Pod enters **CrashLoopBackOff** when its container starts, crashes, and Kubernetes repeatedly restarts it.

### ðŸ”¹ Common Causes

1. **Misconfiguration**

   * Wrong ConfigMap/Secret mounted
   * Missing or incorrect environment variables

2. **Health Check Failures**

   * Wrong **liveness/readiness probe** path or port
   * Application takes longer to start than probe timeout

3. **Application Dependency Missing**

   * e.g., App container depends on DB or API which is not ready

4. **Insufficient Resources**

   * Not enough CPU/memory to start â†’ container gets OOMKilled
   * Node pressure causes eviction

---

### ðŸ”¹ How to Debug

| **Check**              | **Command**                          | **Purpose**                                                  |
| ---------------------- | ------------------------------------ | ------------------------------------------------------------ |
| Pod placement & events | `kubectl describe pod <pod-name>`    | See scheduling errors, init container status, probe failures |
| Container logs         | `kubectl logs <pod-name>`            | Check runtime errors                                         |
| Previous crash logs    | `kubectl logs <pod-name> --previous` | Logs from previous crashed container                         |
| Resource status        | `kubectl top pod <pod-name>`         | Check CPU/memory usage                                       |

---

### ðŸ”¹ Why Pods Keep Restarting / Disappearing

* If a **Deployment** or **ReplicaSet** controls the Pod:

  * Kubernetes deletes failing Pods and creates new ones to meet the replica count.
* If the **container fails due to low resources (OOMKilled)**:

  * Pod restarts continuously.
* If the **node itself is under pressure**:

  * Kubelet evicts Pods and reschedules them.

So:

* Continuous restarts **inside the same Pod** â†’ CrashLoopBackOff
* Continuous **Pod deletion & recreation** â†’ Controlled by Deployment/ReplicaSet trying to heal

---

### ðŸ”¹ Key Differentiation

* **CrashLoopBackOff** = container repeatedly failing inside the same Pod
* **New Pods created** = Deployment/ReplicaSet replacing failed Pods

---